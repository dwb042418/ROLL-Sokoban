hydra:
  run:
    dir: .
  output_subdir: null

exp_name: "sokoban_sft_baseline"
seed: 42
logging_dir: ./output/logs
output_dir: ./output
system_envs:
  USE_MODELSCOPE: '1'

track_with: tensorboard
tracker_kwargs:
  log_dir: ./output/tensorboard

num_gpus_per_node: 1

save_steps: 200
logging_steps: 10
eval_steps: 200
resume_from_checkpoint: false

sequence_length: 512

pretrain: Qwen/Qwen2-1.5B-Instruct
# pretrain: Qwen/Qwen3-4B-Instruct-2507
# sft related
prompt_key: instruction
query_key: null
response_key: output

# Temporarily disable validation due to GPU memory constraints
# Training uses ~76GB, leaving insufficient memory for validation
# validation:
#   data_args:
#     file_name: data/sft/sokoban_val_io.jsonl
#     template: qwen3
#     preprocessing_num_workers: 4

sft_train:
  model_args:
    dtype: bf16
    trust_remote_code: true
    use_flash_attn: false
    gradient_checkpointing: true
  training_args:
    num_train_epochs: 10
    per_device_train_batch_size: 1
    gradient_accumulation_steps: 16
    learning_rate: 5.0e-5
    weight_decay: 0.01
    warmup_ratio: 0.03
    max_steps: 2000
    logging_steps: 10
    save_steps: 200
    eval_steps: 200
    save_total_limit: 3
    max_grad_norm: 1.0
    bf16: true
    additional_configs:
      transformer_impl: local
  data_args:
    file_name: data/sft/sokoban_train_io.jsonl
    template: qwen2_5
    preprocessing_num_workers: 4
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 1
      pipeline_model_parallel_size: 1
      sequence_parallel: true
      use_distributed_optimizer: true
  device_mapping: '[0]'
  infer_batch_size: 2
